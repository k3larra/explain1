### Comparing Grad-Cam explanations with human explanations 

This repo supports the paper _"Breaking out of the interpretability asylum using social sciences and coffee plates as battering ram."_ with code and data in the form of images. The code can be found in the file **code.ipynb** and images used are in the compressed file **images/data_2.zip**.

We use an implementation of [GradCam](https://github.com/anhquan0412/animation-classification) by Quan Tran. GradCam produces visual explanations by highlighting areas central to the decision making. 

Read more about GradCam in the paper _"Grad-cam: Visual explanations from deep networks via gradient-based localization"_ written by Selvaju et al.

For machine learning we use the framework by [Fast.ai](fast.ai). 
