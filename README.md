### Comparing Grad-Cam explanations with human explanations 

This repository supports the paper _"Breaking out of the interpretability asylum using social sciences and coffee plates as battering ram."_. The code can be found in the file **code.ipynb** and images used are in the compressed file **images/data_2.zip**.

We use an implementation of [GradCam](https://github.com/anhquan0412/animation-classification) by Quan Tran. GradCam produces visual explanations by highlighting areas central to the decision making. Read more about GradCam in the paper _"Grad-cam: Visual explanations from deep networks via gradient-based localization"_ written by Selvaju et al.

For machine learning part of the work we used the framework by [Fast.ai](fast.ai). 
